{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.types import StructField, FloatType\r\n",
    "from pyspark.sql.window import Window\r\n",
    "from pyspark import keyword_only\r\n",
    "from pyspark.ml import Transformer, Estimator, Model\r\n",
    "from pyspark.ml.evaluation import Evaluator\r\n",
    "from pyspark.ml.param.shared import HasInputCol, HasInputCols, HasOutputCol, Params, Param, TypeConverters, HasLabelCol, HasPredictionCol, HasFeaturesCol, HasThreshold\r\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable, JavaMLReadable, JavaMLWritable\r\n",
    "from pyspark.sql.types import *\r\n",
    "from pyspark.ml import Pipeline\r\n",
    "from pyspark.ml.feature import VectorAssembler\r\n",
    "from pyspark.ml.regression import GBTRegressor, _GBTRegressorParams, GBTRegressionModel\r\n",
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\r\n",
    "    .builder \\\r\n",
    "    .appName(\"geotab\") \\\r\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"../../data/raw/train.csv\"\r\n",
    "file_type = \"csv\"\r\n",
    "\r\n",
    "# CSV options\r\n",
    "infer_schema = \"true\"\r\n",
    "first_row_is_header = \"true\"\r\n",
    "delimiter = \",\"\r\n",
    "\r\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\r\n",
    "spark_df = spark.read.format(file_type) \\\r\n",
    "  .option(\"inferSchema\", infer_schema) \\\r\n",
    "  .option(\"header\", first_row_is_header) \\\r\n",
    "  .option(\"sep\", delimiter) \\\r\n",
    "  .load(file_location)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['RowId',\n 'IntersectionId',\n 'Latitude',\n 'Longitude',\n 'EntryStreetName',\n 'ExitStreetName',\n 'EntryHeading',\n 'ExitHeading',\n 'Hour',\n 'Weekend',\n 'Month',\n 'Path',\n 'TotalTimeStopped_p20',\n 'TotalTimeStopped_p40',\n 'TotalTimeStopped_p50',\n 'TotalTimeStopped_p60',\n 'TotalTimeStopped_p80',\n 'TimeFromFirstStop_p20',\n 'TimeFromFirstStop_p40',\n 'TimeFromFirstStop_p50',\n 'TimeFromFirstStop_p60',\n 'TimeFromFirstStop_p80',\n 'DistanceToFirstStop_p20',\n 'DistanceToFirstStop_p40',\n 'DistanceToFirstStop_p50',\n 'DistanceToFirstStop_p60',\n 'DistanceToFirstStop_p80',\n 'City']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(856387, 28)\n"
     ]
    }
   ],
   "source": [
    "print((spark_df.count(), len(spark_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|IntersectionId|\n",
      "+--------------+\n",
      "|           471|\n",
      "|           496|\n",
      "|           148|\n",
      "|           463|\n",
      "|          1238|\n",
      "|           833|\n",
      "|          1088|\n",
      "|          1342|\n",
      "|          1580|\n",
      "|          1591|\n",
      "|          1645|\n",
      "|          1829|\n",
      "|          1959|\n",
      "|          2122|\n",
      "|          2142|\n",
      "|          2366|\n",
      "|          2659|\n",
      "|          2866|\n",
      "|           392|\n",
      "|           243|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select('IntersectionId').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------+---------+---------------+--------------+------------+-----------+----+-------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+----+\n",
      "|RowId|IntersectionId|Latitude|Longitude|EntryStreetName|ExitStreetName|EntryHeading|ExitHeading|Hour|Weekend|Month|Path|TotalTimeStopped_p20|TotalTimeStopped_p40|TotalTimeStopped_p50|TotalTimeStopped_p60|TotalTimeStopped_p80|TimeFromFirstStop_p20|TimeFromFirstStop_p40|TimeFromFirstStop_p50|TimeFromFirstStop_p60|TimeFromFirstStop_p80|DistanceToFirstStop_p20|DistanceToFirstStop_p40|DistanceToFirstStop_p50|DistanceToFirstStop_p60|DistanceToFirstStop_p80|City|\n",
      "+-----+--------------+--------+---------+---------------+--------------+------------+-----------+----+-------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+----+\n",
      "|    0|             0|       0|        0|              0|             0|           0|          0|   0|      0|    0|   0|                   0|                   0|                   0|                   0|                   0|                    0|                    0|                    0|                    0|                    0|                      0|                      0|                      0|                      0|                      0|   0|\n",
      "+-----+--------------+--------+---------+---------------+--------------+------------+-----------+----+-------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select([F.count(F.when(F.isnan(c), c)).alias(c) for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('RowId', 'int'),\n ('IntersectionId', 'int'),\n ('Latitude', 'double'),\n ('Longitude', 'double'),\n ('EntryStreetName', 'string'),\n ('ExitStreetName', 'string'),\n ('EntryHeading', 'string'),\n ('ExitHeading', 'string'),\n ('Hour', 'int'),\n ('Weekend', 'int'),\n ('Month', 'int'),\n ('Path', 'string'),\n ('TotalTimeStopped_p20', 'double'),\n ('TotalTimeStopped_p40', 'double'),\n ('TotalTimeStopped_p50', 'double'),\n ('TotalTimeStopped_p60', 'double'),\n ('TotalTimeStopped_p80', 'double'),\n ('TimeFromFirstStop_p20', 'double'),\n ('TimeFromFirstStop_p40', 'double'),\n ('TimeFromFirstStop_p50', 'double'),\n ('TimeFromFirstStop_p60', 'double'),\n ('TimeFromFirstStop_p80', 'double'),\n ('DistanceToFirstStop_p20', 'double'),\n ('DistanceToFirstStop_p40', 'double'),\n ('DistanceToFirstStop_p50', 'double'),\n ('DistanceToFirstStop_p60', 'double'),\n ('DistanceToFirstStop_p80', 'double'),\n ('City', 'string')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(RowId=0.0, IntersectionId=0.0, Latitude=0.0, Longitude=0.0, EntryStreetName=0.00951439010634211, ExitStreetName=0.007341307142681989, EntryHeading=0.0, ExitHeading=0.0, Hour=0.0, Weekend=0.0, Month=0.0, Path=0.0, TotalTimeStopped_p20=0.0, TotalTimeStopped_p40=0.0, TotalTimeStopped_p50=0.0, TotalTimeStopped_p60=0.0, TotalTimeStopped_p80=0.0, TimeFromFirstStop_p20=0.0, TimeFromFirstStop_p40=0.0, TimeFromFirstStop_p50=0.0, TimeFromFirstStop_p60=0.0, TimeFromFirstStop_p80=0.0, DistanceToFirstStop_p20=0.0, DistanceToFirstStop_p40=0.0, DistanceToFirstStop_p50=0.0, DistanceToFirstStop_p60=0.0, DistanceToFirstStop_p80=0.0, City=0.0)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select([(F.count(F.when(F.col(c).isNull(), c))/spark_df.count()).alias(c) for c in spark_df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullThresholdRemover(Transformer, HasThreshold, HasInputCols, DefaultParamsReadable, DefaultParamsWritable):\r\n",
    "\r\n",
    "    @keyword_only\r\n",
    "    def __init__(self, inputCols=None, threshold=0.3) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self._setDefault(inputCols=inputCols, Colsthreshold=threshold)\r\n",
    "        kwargs = self._input_kwargs\r\n",
    "        self.setParams(**kwargs)\r\n",
    "    \r\n",
    "    @keyword_only\r\n",
    "    def setParams(self, inputCols=None, threshold=0.3):\r\n",
    "        kwargs = self._input_kwargs\r\n",
    "        self._set(**kwargs)\r\n",
    "    \r\n",
    "    def _transform(self, dataset):\r\n",
    "        threshold = self.getThreshold()\r\n",
    "        cols = dataset.columns\r\n",
    "        datasetRowCount = dataset.count()\r\n",
    "        inputCols = list(set(self.g))\r\n",
    "        \r\n",
    "        colsNullCount = dataset.select([(F.count(F.when(F.col(c).isNull(), c))/datasetRowCount).alias(c) for c in dataset.columns]).collect()\r\n",
    "\r\n",
    "        colsNullCount = [row.asDict() for row in aggregated_row]\r\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python368jvsc74a57bd07250dc283eb3810f2d9feed054da14ba567bea79ccbaa8314387438e626e0f74"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "7250dc283eb3810f2d9feed054da14ba567bea79ccbaa8314387438e626e0f74"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}